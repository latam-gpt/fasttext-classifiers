# FastText Classifiers

This repository provides an alternative approach to the second step of the methodology proposed in the [llm-data-eval](https://github.com/latam-gpt/llm-data-eval) repository.

## Training the Encoder

All training operations are contained in `train_fasttext_eval.py`.

### Prerequisites

Before training, we need a sample dataset (from the Hugging Face library) with two columns:
1. **text** – The input text.
2. **label** – The label generated by an LLM model.

We use this dataset to train our encoder model.

### Experiment Details

For this experiment, we generated **500k prompts** using Mixtral. The encoder model used is **fastText**.
Since fastText converges quickly, we use **30k samples** for training. This ensures a **balanced dataset** with normalized data.

### Training Command

To train the model, use the following command:

```bash
python train_fasttext_eval.py \
  --model_path path/to/saved_model \
  --dataset_path path/to/dataframe \
  --num_epochs 25 \
  --size 30000 \
  --normalize True
```
